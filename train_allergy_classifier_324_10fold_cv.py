#!/usr/bin/env python3
"""
è®­ç»ƒ324æ•°æ®é›†çš„å¤šæ¨¡æ€åˆ†ç±»å™¨ - 10æŠ˜äº¤å‰éªŒè¯
ä½¿ç”¨source 115æ•°æ®é›† (108ä¸ªæ ·æœ¬) è¿›è¡Œ10æŠ˜äº¤å‰éªŒè¯: 
- åªä½¿ç”¨ 324_X_train_source115_genus_metabolite.xlsx (108ä¸ªæ ·æœ¬)
- 10æŠ˜äº¤å‰éªŒè¯ï¼šæ¯æŠ˜çº¦10-11ä¸ªæ ·æœ¬ä½œä¸ºæµ‹è¯•é›†ï¼Œå…¶ä½™ä½œä¸ºè®­ç»ƒé›†
- ä¸ä½¿ç”¨source 203æ•°æ®

æ•°æ®åˆ†å‰²ç­–ç•¥:
- Source 115 (108ä¸ªæ ·æœ¬) -> 10æŠ˜äº¤å‰éªŒè¯ (æ¯æŠ˜: ~97ä¸ªè®­ç»ƒ + ~11ä¸ªæµ‹è¯•)
- æ¯æŠ˜å†…éƒ¨å†åˆ†: è®­ç»ƒé›† (~78ä¸ª) + éªŒè¯é›† (~19ä¸ª) + æµ‹è¯•é›† (~11ä¸ª)

æ ‡ç­¾å«ä¹‰:
- 1 (IgE): å¼•èµ·è¿‡æ•ååº”çš„æ ·æœ¬
- 0 (N group): éè¿‡æ•æ€§æ ·æœ¬ï¼ˆæ­£å¸¸å¯¹ç…§ç»„ï¼‰
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset, WeightedRandomSampler

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import StratifiedKFold, train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import roc_auc_score, classification_report, confusion_matrix, roc_curve
from sklearn.utils.class_weight import compute_class_weight

import warnings
warnings.filterwarnings('ignore')

torch.manual_seed(42)
np.random.seed(42)

class EnhancedMultimodalClassifier(nn.Module):
    """å¢å¼ºçš„å¤šæ¨¡æ€åˆ†ç±»å™¨ - æ³¨æ„åŠ›å¼•å¯¼çš„ç‰¹å¾èåˆ"""
    def __init__(self, micro_input_dim, metab_input_dim, hidden_dim=128, dropout_rate=0.4):
        super(EnhancedMultimodalClassifier, self).__init__()
        
        self.micro_input_dim = micro_input_dim
        self.metab_input_dim = metab_input_dim
        
        # å¾®ç”Ÿç‰©ç»„è·¯å¾„ (Genus features pathway)
        self.micro_net = nn.Sequential(
            nn.Linear(micro_input_dim, hidden_dim),
            nn.BatchNorm1d(hidden_dim),
            nn.ReLU(),
            nn.Dropout(dropout_rate),
            
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.BatchNorm1d(hidden_dim // 2),
            nn.ReLU(),
            nn.Dropout(dropout_rate * 0.7),
            
            nn.Linear(hidden_dim // 2, 32),
            nn.ReLU(),
            nn.Dropout(dropout_rate * 0.5)
        )
        
        # ä»£è°¢ç‰©è·¯å¾„ (Metabolite features pathway)
        self.metab_net = nn.Sequential(
            nn.Linear(metab_input_dim, hidden_dim),
            nn.BatchNorm1d(hidden_dim),
            nn.ReLU(),
            nn.Dropout(dropout_rate),
            
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.BatchNorm1d(hidden_dim // 2),
            nn.ReLU(),
            nn.Dropout(dropout_rate * 0.7),
            
            nn.Linear(hidden_dim // 2, 32),
            nn.ReLU(),
            nn.Dropout(dropout_rate * 0.5)
        )
        
        # æ³¨æ„åŠ›æœºåˆ¶ (Attention mechanism for interpretability)
        self.attention = nn.Sequential(
            nn.Linear(64, 32),
            nn.Tanh(),
            nn.Linear(32, 2),
            nn.Softmax(dim=1)
        )
        
        # æœ€ç»ˆåˆ†ç±»å™¨ (Final classifier)
        self.classifier = nn.Sequential(
            nn.Linear(64, 64),
            nn.BatchNorm1d(64),
            nn.ReLU(),
            nn.Dropout(dropout_rate),
            
            nn.Linear(64, 32),
            nn.BatchNorm1d(32),
            nn.ReLU(),
            nn.Dropout(dropout_rate * 0.7),
            
            nn.Linear(32, 16),
            nn.ReLU(),
            nn.Dropout(dropout_rate * 0.5),
            
            nn.Linear(16, 1),
            nn.Sigmoid()
        )
        
    def forward(self, x_micro, x_metab):
        # å¤„ç†å¾®ç”Ÿç‰©ç»„ç‰¹å¾
        micro_features = self.micro_net(x_micro)
        # å¤„ç†ä»£è°¢ç‰©ç‰¹å¾
        metab_features = self.metab_net(x_metab)
        
        # ç‰¹å¾ä¸²è”
        combined = torch.cat([micro_features, metab_features], dim=1)
        
        # è®¡ç®—æ³¨æ„åŠ›æƒé‡
        attention_weights = self.attention(combined)
        micro_weight = attention_weights[:, 0:1]
        metab_weight = attention_weights[:, 1:2]
        
        # åŠ æƒç‰¹å¾ç»„åˆ
        weighted_features = torch.cat([
            micro_features * micro_weight,
            metab_features * metab_weight
        ], dim=1)
        
        # æœ€ç»ˆåˆ†ç±»
        output = self.classifier(weighted_features)
        
        return {
            'classification': output,
            'micro_features': micro_features,
            'metab_features': metab_features,
            'attention_weights': attention_weights,
            'combined_features': weighted_features
        }

def clean_column_names(df):
    """æ¸…ç†åˆ—åï¼Œç§»é™¤ç‰¹æ®Šå­—ç¬¦"""
    df = df.copy()
    df.columns = df.columns.str.replace('[', '', regex=False)
    df.columns = df.columns.str.replace(']', '', regex=False)
    df.columns = df.columns.str.replace('<', '_lt_', regex=False)
    df.columns = df.columns.str.replace('>', '_gt_', regex=False)
    df.columns = df.columns.str.replace('(', '_', regex=False)
    df.columns = df.columns.str.replace(')', '_', regex=False)
    df.columns = df.columns.str.replace(' ', '_', regex=False)
    df.columns = df.columns.str.replace('-', '_', regex=False)
    df.columns = df.columns.str.replace('/', '_', regex=False)
    df.columns = df.columns.str.replace('\\', '_', regex=False)
    df.columns = df.columns.str.replace('_+', '_', regex=True)
    df.columns = df.columns.str.strip('_')
    return df

def create_data_loader(X_micro, X_metab, y, batch_size=16, shuffle=True, sampler=None):
    """åˆ›å»ºPyTorchæ•°æ®åŠ è½½å™¨"""
    X_micro_tensor = torch.FloatTensor(X_micro)
    X_metab_tensor = torch.FloatTensor(X_metab)
    y_tensor = torch.FloatTensor(y).unsqueeze(1)
    
    dataset = TensorDataset(X_micro_tensor, X_metab_tensor, y_tensor)
    
    if sampler is not None:
        return DataLoader(dataset, batch_size=batch_size, sampler=sampler)
    else:
        return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)

def evaluate_model(model, data_loader, device):
    """è¯„ä¼°æ¨¡å‹æ€§èƒ½"""
    model.eval()
    all_predictions = []
    all_labels = []
    
    with torch.no_grad():
        for x_micro, x_metab, labels in data_loader:
            x_micro, x_metab, labels = x_micro.to(device), x_metab.to(device), labels.to(device)
            outputs = model(x_micro, x_metab)
            predictions = outputs['classification'].cpu().numpy().flatten()
            
            all_predictions.extend(predictions)
            all_labels.extend(labels.cpu().numpy().flatten())
    
    if len(all_predictions) == 0:
        return {'auc': 0.0, 'accuracy': 0.0, 'predictions': np.array([]), 'labels': np.array([])}
    
    auc = roc_auc_score(all_labels, all_predictions)
    binary_pred = (np.array(all_predictions) > 0.5).astype(int)
    accuracy = (binary_pred == np.array(all_labels)).mean()
    
    return {
        'auc': auc,
        'accuracy': accuracy,
        'predictions': np.array(all_predictions),
        'labels': np.array(all_labels)
    }

def train_single_fold(X_train_genus, X_train_metab, y_train, X_val_genus, X_val_metab, y_val, 
                     X_test_genus, X_test_metab, y_test, device, fold_num):
    """è®­ç»ƒå•ä¸ªæŠ˜å """
    
    # åˆ›å»ºæ¨¡å‹
    model = EnhancedMultimodalClassifier(
        micro_input_dim=X_train_genus.shape[1],
        metab_input_dim=X_train_metab.shape[1],
        hidden_dim=64,  # å‡å°æ¨¡å‹ä»¥é€‚åº”å°æ•°æ®é›†
        dropout_rate=0.3
    ).to(device)
    
    # å¤„ç†ç±»åˆ«ä¸å¹³è¡¡
    class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)
    class_weight_dict = {0: class_weights[0], 1: class_weights[1]}
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    sample_weights = [class_weight_dict[label] for label in y_train]
    sampler = WeightedRandomSampler(sample_weights, len(sample_weights))
    
    train_loader = create_data_loader(X_train_genus, X_train_metab, y_train, 
                                    batch_size=8, sampler=sampler)  # å°batch size
    val_loader = create_data_loader(X_val_genus, X_val_metab, y_val, 
                                  batch_size=8, shuffle=False)
    test_loader = create_data_loader(X_test_genus, X_test_metab, y_test, 
                                   batch_size=8, shuffle=False)
    
    # ä¼˜åŒ–å™¨
    optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-3)
    criterion = nn.BCELoss(weight=torch.FloatTensor([class_weights[1]]).to(device))
    
    # è®­ç»ƒå¾ªç¯
    best_val_auc = 0
    patience = 10
    no_improve_count = 0
    
    for epoch in range(50):  # å‡å°‘è®­ç»ƒè½®æ•°
        # è®­ç»ƒé˜¶æ®µ
        model.train()
        epoch_loss = 0
        num_batches = 0
        
        for x_genus, x_metab, labels in train_loader:
            x_genus, x_metab, labels = x_genus.to(device), x_metab.to(device), labels.to(device)
            
            optimizer.zero_grad()
            outputs = model(x_genus, x_metab)
            
            loss = criterion(outputs['classification'], labels)
            l2_reg = sum(p.pow(2.0).sum() for p in model.parameters())
            total_loss = loss + 0.01 * l2_reg  # å¢åŠ æ­£åˆ™åŒ–
            
            total_loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            optimizer.step()
            
            epoch_loss += loss.item()
            num_batches += 1
        
        # éªŒè¯é˜¶æ®µ
        if len(y_val) > 0:
            val_results = evaluate_model(model, val_loader, device)
            val_auc = val_results['auc']
            
            if val_auc > best_val_auc:
                best_val_auc = val_auc
                best_model_state = model.state_dict().copy()
                no_improve_count = 0
            else:
                no_improve_count += 1
                
            if no_improve_count >= patience:
                break
        else:
            # å¦‚æœæ²¡æœ‰éªŒè¯é›†ï¼Œä¿å­˜æœ€åçš„æ¨¡å‹
            best_model_state = model.state_dict().copy()
    
    # åŠ è½½æœ€ä½³æ¨¡å‹å¹¶è¯„ä¼°æµ‹è¯•é›†
    model.load_state_dict(best_model_state)
    test_results = evaluate_model(model, test_loader, device)
    
    # è·å–æ³¨æ„åŠ›æƒé‡
    model.eval()
    attention_weights_list = []
    with torch.no_grad():
        for x_genus, x_metab, _ in test_loader:
            x_genus, x_metab = x_genus.to(device), x_metab.to(device)
            outputs = model(x_genus, x_metab)
            attention_weights_list.append(outputs['attention_weights'].cpu().numpy())
    
    if attention_weights_list:
        attention_weights = np.vstack(attention_weights_list)
    else:
        attention_weights = np.array([[0.5, 0.5]])  # é»˜è®¤æƒé‡
    
    return {
        'test_auc': test_results['auc'],
        'test_accuracy': test_results['accuracy'],
        'test_predictions': test_results['predictions'],
        'test_labels': test_results['labels'],
        'attention_weights': attention_weights,
        'best_val_auc': best_val_auc
    }

def plot_cv_results(cv_results):
    """ç»˜åˆ¶äº¤å‰éªŒè¯ç»“æœ"""
    fig, axes = plt.subplots(2, 2, figsize=(15, 12))
    
    # 1. æ¯æŠ˜AUCåˆ†å¸ƒ
    fold_aucs = [result['test_auc'] for result in cv_results]
    axes[0,0].bar(range(1, len(fold_aucs)+1), fold_aucs, alpha=0.7, color='steelblue')
    axes[0,0].axhline(y=np.mean(fold_aucs), color='red', linestyle='--', 
                     label=f'Mean AUC: {np.mean(fold_aucs):.3f}')
    axes[0,0].set_xlabel('Fold')
    axes[0,0].set_ylabel('Test AUC')
    axes[0,0].set_title('10-Fold Cross-Validation AUC Results')
    axes[0,0].legend()
    axes[0,0].grid(True, alpha=0.3)
    
    # 2. AUCåˆ†å¸ƒç›´æ–¹å›¾
    axes[0,1].hist(fold_aucs, bins=8, alpha=0.7, color='lightblue', edgecolor='black')
    axes[0,1].axvline(x=np.mean(fold_aucs), color='red', linestyle='--', 
                     label=f'Mean: {np.mean(fold_aucs):.3f}')
    axes[0,1].axvline(x=np.median(fold_aucs), color='green', linestyle='--', 
                     label=f'Median: {np.median(fold_aucs):.3f}')
    axes[0,1].set_xlabel('AUC Score')
    axes[0,1].set_ylabel('Frequency')
    axes[0,1].set_title('Distribution of AUC Scores Across Folds')
    axes[0,1].legend()
    axes[0,1].grid(True, alpha=0.3)
    
    # 3. æ³¨æ„åŠ›æƒé‡åˆ†æ
    all_attention = np.vstack([result['attention_weights'] for result in cv_results if len(result['attention_weights']) > 0])
    avg_genus_attention = all_attention[:, 0].mean()
    avg_metab_attention = all_attention[:, 1].mean()
    
    modalities = ['Genus\\n(Microbiome)', 'Metabolite']
    avg_weights = [avg_genus_attention, avg_metab_attention]
    colors = ['lightcoral', 'lightblue']
    
    bars = axes[1,0].bar(modalities, avg_weights, color=colors, alpha=0.8)
    axes[1,0].set_ylabel('Average Attention Weight')
    axes[1,0].set_title('Average Attention Weights Across All Folds')
    axes[1,0].set_ylim(0, 1)
    axes[1,0].grid(axis='y', alpha=0.3)
    
    for bar, weight in zip(bars, avg_weights):
        axes[1,0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02, 
                      f'{weight:.3f}', ha='center', va='bottom', fontweight='bold')
    
    # 4. ç»Ÿè®¡æ±‡æ€»
    axes[1,1].axis('off')
    stats_text = f"""
10-Fold Cross-Validation Results Summary

AUC Statistics:
â€¢ Mean AUC: {np.mean(fold_aucs):.4f} Â± {np.std(fold_aucs):.4f}
â€¢ Median AUC: {np.median(fold_aucs):.4f}
â€¢ Min AUC: {np.min(fold_aucs):.4f}
â€¢ Max AUC: {np.max(fold_aucs):.4f}
â€¢ Range: {np.max(fold_aucs) - np.min(fold_aucs):.4f}

Model Attention:
â€¢ Genus (Microbiome): {avg_genus_attention:.1%}
â€¢ Metabolite: {avg_metab_attention:.1%}

Dataset:
â€¢ Total Samples: 108
â€¢ Each Fold Test Size: ~11 samples
â€¢ Features: 891 (567 genus + 324 metabolite)
"""
    axes[1,1].text(0.1, 0.9, stats_text, transform=axes[1,1].transAxes, 
                  fontsize=11, verticalalignment='top', fontfamily='monospace',
                  bbox=dict(boxstyle="round,pad=0.5", facecolor="lightgray", alpha=0.8))
    
    plt.tight_layout()
    plt.savefig('10_fold_cv_results_source115.png', dpi=300, bbox_inches='tight')
    plt.show()

def main():
    print("ğŸ§¬ å¤šæ¨¡æ€è¿‡æ•æ€§ç–¾ç—…åˆ†ç±»å™¨ - 10æŠ˜äº¤å‰éªŒè¯")
    print("=" * 80)
    print("ğŸ“‹ éªŒè¯ç­–ç•¥:")
    print("   â€¢ åªä½¿ç”¨ Source 115 æ•°æ®é›† (108ä¸ªæ ·æœ¬)")
    print("   â€¢ 10æŠ˜äº¤å‰éªŒè¯: æ¯æŠ˜~11ä¸ªæµ‹è¯•æ ·æœ¬, ~97ä¸ªè®­ç»ƒ+éªŒè¯æ ·æœ¬")
    print("   â€¢ æ¯æŠ˜å†…éƒ¨: ~78ä¸ªè®­ç»ƒ + ~19ä¸ªéªŒè¯ + ~11ä¸ªæµ‹è¯•")
    print("   â€¢ æ ‡ç­¾: 1 (IgEè¿‡æ•), 0 (Næ­£å¸¸)")
    print("=" * 80)
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"\\nğŸ’» ä½¿ç”¨è®¾å¤‡: {device}")
    
    # åŠ è½½æ•°æ® - åªä½¿ç”¨source 115
    print("\\nğŸ“Š åŠ è½½æ•°æ®...")
    try:
        X_data = pd.read_excel('324_X_train_source115_genus_metabolite.xlsx', index_col=0)
        y_data = pd.read_excel('324_y_train_source115_genus_metabolite.xlsx', index_col=0)
        
        print(f"âœ… æ•°æ®åŠ è½½æˆåŠŸ")
        print(f"   æ€»æ ·æœ¬æ•°: {X_data.shape[0]}ä¸ª")
        print(f"   ç‰¹å¾æ•°: {X_data.shape[1]}ä¸ª")
        
    except FileNotFoundError as e:
        print(f"âŒ æ•°æ®æ–‡ä»¶æœªæ‰¾åˆ°: {e}")
        return
    
    # æ¸…ç†åˆ—å
    X_data = clean_column_names(X_data)
    y_values = y_data.iloc[:, 0].values
    
    print(f"\\nğŸ“ˆ æ•°æ®ç±»åˆ«åˆ†å¸ƒ:")
    class_counts = np.bincount(y_values)
    print(f"   éè¿‡æ•æ ·æœ¬ (N): {class_counts[0]}ä¸ª ({class_counts[0]/len(y_values)*100:.1f}%)")
    print(f"   è¿‡æ•æ ·æœ¬ (IgE): {class_counts[1]}ä¸ª ({class_counts[1]/len(y_values)*100:.1f}%)")
    
    # åˆ†ç¦»å¤šæ¨¡æ€ç‰¹å¾
    genus_features = [col for col in X_data.columns if 'genus' in col.lower()]
    metabolite_features = [col for col in X_data.columns if 'metabolite' in col.lower()]
    
    X_genus = X_data[genus_features].values
    X_metab = X_data[metabolite_features].values
    
    print(f"\\nğŸ”¬ ç‰¹å¾åˆ†æ:")
    print(f"   Genusç‰¹å¾: {len(genus_features)}ä¸ª")
    print(f"   Metaboliteç‰¹å¾: {len(metabolite_features)}ä¸ª")
    print(f"   æ€»ç‰¹å¾æ•°: {len(genus_features) + len(metabolite_features)}ä¸ª")
    
    # 10æŠ˜äº¤å‰éªŒè¯
    print("\\nğŸ”„ å¼€å§‹10æŠ˜äº¤å‰éªŒè¯...")
    skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)
    cv_results = []
    
    for fold, (train_val_idx, test_idx) in enumerate(skf.split(X_genus, y_values)):
        print(f"\\n--- ç¬¬ {fold+1}/10 æŠ˜ ---")
        
        # åˆ†å‰²æ•°æ®
        X_train_val_genus, X_test_fold_genus = X_genus[train_val_idx], X_genus[test_idx]
        X_train_val_metab, X_test_fold_metab = X_metab[train_val_idx], X_metab[test_idx]
        y_train_val, y_test_fold = y_values[train_val_idx], y_values[test_idx]
        
        # è¿›ä¸€æ­¥åˆ†å‰²è®­ç»ƒéªŒè¯é›†
        if len(y_train_val) > 10:  # åªæœ‰è¶³å¤Ÿæ ·æœ¬æ—¶æ‰åˆ†å‰²éªŒè¯é›†
            X_train_genus, X_val_genus, X_train_metab, X_val_metab, y_train, y_val = train_test_split(
                X_train_val_genus, X_train_val_metab, y_train_val, 
                test_size=0.2, random_state=42, stratify=y_train_val
            )
        else:
            # æ ·æœ¬å¤ªå°‘ï¼Œä¸åˆ†éªŒè¯é›†
            X_train_genus, X_val_genus = X_train_val_genus, np.array([]).reshape(0, X_train_val_genus.shape[1])
            X_train_metab, X_val_metab = X_train_val_metab, np.array([]).reshape(0, X_train_val_metab.shape[1])
            y_train, y_val = y_train_val, np.array([])
        
        print(f"   è®­ç»ƒé›†: {len(y_train)}ä¸ªæ ·æœ¬")
        print(f"   éªŒè¯é›†: {len(y_val)}ä¸ªæ ·æœ¬")
        print(f"   æµ‹è¯•é›†: {len(y_test_fold)}ä¸ªæ ·æœ¬")
        
        # æ•°æ®æ ‡å‡†åŒ–
        scaler_genus = StandardScaler()
        scaler_metab = StandardScaler()
        
        X_train_genus_scaled = scaler_genus.fit_transform(X_train_genus)
        X_train_metab_scaled = scaler_metab.fit_transform(X_train_metab)
        
        if len(y_val) > 0:
            X_val_genus_scaled = scaler_genus.transform(X_val_genus)
            X_val_metab_scaled = scaler_metab.transform(X_val_metab)
        else:
            X_val_genus_scaled = np.array([]).reshape(0, X_train_genus_scaled.shape[1])
            X_val_metab_scaled = np.array([]).reshape(0, X_train_metab_scaled.shape[1])
        
        X_test_fold_genus_scaled = scaler_genus.transform(X_test_fold_genus)
        X_test_fold_metab_scaled = scaler_metab.transform(X_test_fold_metab)
        
        # è®­ç»ƒæ¨¡å‹
        fold_result = train_single_fold(
            X_train_genus_scaled, X_train_metab_scaled, y_train,
            X_val_genus_scaled, X_val_metab_scaled, y_val,
            X_test_fold_genus_scaled, X_test_fold_metab_scaled, y_test_fold,
            device, fold+1
        )
        
        cv_results.append(fold_result)
        print(f"   æµ‹è¯•AUC: {fold_result['test_auc']:.4f}")
        print(f"   æµ‹è¯•å‡†ç¡®ç‡: {fold_result['test_accuracy']:.4f}")
    
    # æ±‡æ€»ç»“æœ
    print("\\n" + "="*80)
    print("ğŸ¯ 10æŠ˜äº¤å‰éªŒè¯æ€»ç»“")
    print("="*80)
    
    fold_aucs = [result['test_auc'] for result in cv_results]
    fold_accs = [result['test_accuracy'] for result in cv_results]
    
    print(f"ğŸ”¥ å¹³å‡æµ‹è¯•AUC: {np.mean(fold_aucs):.4f} Â± {np.std(fold_aucs):.4f}")
    print(f"ğŸ”¥ å¹³å‡æµ‹è¯•å‡†ç¡®ç‡: {np.mean(fold_accs):.4f} Â± {np.std(fold_accs):.4f}")
    print(f"ğŸ”¥ AUCèŒƒå›´: [{np.min(fold_aucs):.4f}, {np.max(fold_aucs):.4f}]")
    print(f"ğŸ”¥ ä¸­ä½æ•°AUC: {np.median(fold_aucs):.4f}")
    
    # æ˜¾ç¤ºæ¯æŠ˜ç»“æœ
    print(f"\\nğŸ“Š å„æŠ˜è¯¦ç»†ç»“æœ:")
    for i, result in enumerate(cv_results):
        print(f"   ç¬¬{i+1:2d}æŠ˜: AUC={result['test_auc']:.4f}, Acc={result['test_accuracy']:.4f}")
    
    # æ³¨æ„åŠ›æƒé‡åˆ†æ
    all_attention = np.vstack([result['attention_weights'] for result in cv_results if len(result['attention_weights']) > 0])
    avg_genus_attention = all_attention[:, 0].mean()
    avg_metab_attention = all_attention[:, 1].mean()
    
    print(f"\\nğŸ¯ å¹³å‡æ³¨æ„åŠ›æƒé‡:")
    print(f"   Genus (å¾®ç”Ÿç‰©ç»„): {avg_genus_attention:.4f} ({avg_genus_attention:.1%})")
    print(f"   Metabolite (ä»£è°¢ç‰©): {avg_metab_attention:.4f} ({avg_metab_attention:.1%})")
    
    # å¯è§†åŒ–ç»“æœ
    print("\\nğŸ“ˆ ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨...")
    plot_cv_results(cv_results)
    
    # ä¿å­˜ç»“æœ
    results_summary = pd.DataFrame({
        'fold': range(1, 11),
        'test_auc': fold_aucs,
        'test_accuracy': fold_accs
    })
    results_summary.to_csv('10_fold_cv_results_summary.csv', index=False)
    
    # ä¿å­˜ç»Ÿè®¡æ±‡æ€»
    stats_summary = pd.DataFrame({
        'metric': ['mean_auc', 'std_auc', 'median_auc', 'min_auc', 'max_auc', 
                  'mean_accuracy', 'std_accuracy', 'avg_genus_attention', 'avg_metabolite_attention'],
        'value': [np.mean(fold_aucs), np.std(fold_aucs), np.median(fold_aucs), 
                 np.min(fold_aucs), np.max(fold_aucs), np.mean(fold_accs), np.std(fold_accs),
                 avg_genus_attention, avg_metab_attention]
    })
    stats_summary.to_csv('cv_statistics_summary.csv', index=False)
    
    print("\\nâœ… 10æŠ˜äº¤å‰éªŒè¯å®Œæˆ!")
    print("ğŸ“ è¾“å‡ºæ–‡ä»¶:")
    print("   â€¢ 10_fold_cv_results_source115.png - äº¤å‰éªŒè¯ç»“æœå¯è§†åŒ–")
    print("   â€¢ 10_fold_cv_results_summary.csv - å„æŠ˜è¯¦ç»†ç»“æœ")
    print("   â€¢ cv_statistics_summary.csv - ç»Ÿè®¡æ±‡æ€»")

if __name__ == "__main__":
    main()
