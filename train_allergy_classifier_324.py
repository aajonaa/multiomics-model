#!/usr/bin/env python3
"""
è®­ç»ƒ324æ•°æ®é›†çš„å¤šæ¨¡æ€åˆ†ç±»å™¨ - è¿‡æ•æ€§ç–¾ç—…åˆ†ç±»
ä½¿ç”¨é¢„åˆ†å‰²çš„train/testæ•°æ®: 
- 324_X_train_source115_genus_metabolite.xlsx (è®­ç»ƒé›†)
- 324_X_test_source203_genus_metabolite.xlsx (æµ‹è¯•é›†)
- 324_y_train_source115_genus_metabolite.xlsx (è®­ç»ƒæ ‡ç­¾)
- 324_y_test_source203_genus_metabolite.xlsx (æµ‹è¯•æ ‡ç­¾        # X_train = pd.read_excel('324_X_train_source115_genus_metabolite.xlsx', index_col=0)
        # y_train = pd.read_excel('324_y_train_source115_genus_metabolite.xlsx', index_col=0)
        # X_test = pd.read_excel('324_X_test_source203_genus_metabolite.xlsx', index_col=0)
        # y_test = pd.read_excel('324_y_test_source203_genus_metabolite.xlsx', index_col=0)

        X_train = pd.read_excel('324_X_test_source203_genus_metabolite.xlsx', index_col=0)
        y_train = pd.read_excel('324_y_test_source203_genus_metabolite.xlsx', index_col=0)
        X_test = pd.read_excel('324_X_train_source115_genus_metabolite.xlsx', index_col=0)
        y_test = pd.read_excel('324_y_train_source115_genus_metabolite.xlsx', index_col=0)
æ ‡ç­¾å«ä¹‰:
- 1 (IgE): å¼•èµ·è¿‡æ•ååº”çš„æ ·æœ¬
- 0 (N group): éè¿‡æ•æ€§æ ·æœ¬ï¼ˆæ­£å¸¸å¯¹ç…§ç»„ï¼‰

ç‰¹å¾: ä½¿ç”¨æ‰€æœ‰891ä¸ªç‰¹å¾ (567ä¸ªgenus + 324ä¸ªmetabolite)
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset, WeightedRandomSampler

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.preprocessing import StandardScaler
from sklearn.metrics import roc_auc_score, classification_report, confusion_matrix, roc_curve
from sklearn.utils.class_weight import compute_class_weight

import warnings
warnings.filterwarnings('ignore')

torch.manual_seed(42)
np.random.seed(42)

class EnhancedMultimodalClassifier(nn.Module):
    """å¢å¼ºçš„å¤šæ¨¡æ€åˆ†ç±»å™¨ - æ³¨æ„åŠ›å¼•å¯¼çš„ç‰¹å¾èåˆ"""
    def __init__(self, micro_input_dim, metab_input_dim, hidden_dim=128, dropout_rate=0.4):
        super(EnhancedMultimodalClassifier, self).__init__()
        
        self.micro_input_dim = micro_input_dim
        self.metab_input_dim = metab_input_dim
        
        # å¾®ç”Ÿç‰©ç»„è·¯å¾„ (Genus features pathway)
        self.micro_net = nn.Sequential(
            nn.Linear(micro_input_dim, hidden_dim),
            nn.BatchNorm1d(hidden_dim),
            nn.ReLU(),
            nn.Dropout(dropout_rate),
            
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.BatchNorm1d(hidden_dim // 2),
            nn.ReLU(),
            nn.Dropout(dropout_rate * 0.7),
            
            nn.Linear(hidden_dim // 2, 32),
            nn.ReLU(),
            nn.Dropout(dropout_rate * 0.5)
        )
        
        # ä»£è°¢ç‰©è·¯å¾„ (Metabolite features pathway)
        self.metab_net = nn.Sequential(
            nn.Linear(metab_input_dim, hidden_dim),
            nn.BatchNorm1d(hidden_dim),
            nn.ReLU(),
            nn.Dropout(dropout_rate),
            
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.BatchNorm1d(hidden_dim // 2),
            nn.ReLU(),
            nn.Dropout(dropout_rate * 0.7),
            
            nn.Linear(hidden_dim // 2, 32),
            nn.ReLU(),
            nn.Dropout(dropout_rate * 0.5)
        )
        
        # æ³¨æ„åŠ›æœºåˆ¶ (Attention mechanism for interpretability)
        self.attention = nn.Sequential(
            nn.Linear(64, 32),
            nn.Tanh(),
            nn.Linear(32, 2),
            nn.Softmax(dim=1)
        )
        
        # æœ€ç»ˆåˆ†ç±»å™¨ (Final classifier)
        self.classifier = nn.Sequential(
            nn.Linear(64, 64),
            nn.BatchNorm1d(64),
            nn.ReLU(),
            nn.Dropout(dropout_rate),
            
            nn.Linear(64, 32),
            nn.BatchNorm1d(32),
            nn.ReLU(),
            nn.Dropout(dropout_rate * 0.7),
            
            nn.Linear(32, 16),
            nn.ReLU(),
            nn.Dropout(dropout_rate * 0.5),
            
            nn.Linear(16, 1),
            nn.Sigmoid()
        )
        
    def forward(self, x_micro, x_metab):
        # å¤„ç†å¾®ç”Ÿç‰©ç»„ç‰¹å¾
        micro_features = self.micro_net(x_micro)
        # å¤„ç†ä»£è°¢ç‰©ç‰¹å¾
        metab_features = self.metab_net(x_metab)
        
        # ç‰¹å¾ä¸²è”
        combined = torch.cat([micro_features, metab_features], dim=1)
        
        # è®¡ç®—æ³¨æ„åŠ›æƒé‡
        attention_weights = self.attention(combined)
        micro_weight = attention_weights[:, 0:1]
        metab_weight = attention_weights[:, 1:2]
        
        # åŠ æƒç‰¹å¾ç»„åˆ
        weighted_features = torch.cat([
            micro_features * micro_weight,
            metab_features * metab_weight
        ], dim=1)
        
        # æœ€ç»ˆåˆ†ç±»
        output = self.classifier(weighted_features)
        
        return {
            'classification': output,
            'micro_features': micro_features,
            'metab_features': metab_features,
            'attention_weights': attention_weights,
            'combined_features': weighted_features
        }

def clean_column_names(df):
    """æ¸…ç†åˆ—åï¼Œç§»é™¤ç‰¹æ®Šå­—ç¬¦"""
    df = df.copy()
    df.columns = df.columns.str.replace('[', '', regex=False)
    df.columns = df.columns.str.replace(']', '', regex=False)
    df.columns = df.columns.str.replace('<', '_lt_', regex=False)
    df.columns = df.columns.str.replace('>', '_gt_', regex=False)
    df.columns = df.columns.str.replace('(', '_', regex=False)
    df.columns = df.columns.str.replace(')', '_', regex=False)
    df.columns = df.columns.str.replace(' ', '_', regex=False)
    df.columns = df.columns.str.replace('-', '_', regex=False)
    df.columns = df.columns.str.replace('/', '_', regex=False)
    df.columns = df.columns.str.replace('\\', '_', regex=False)
    df.columns = df.columns.str.replace('_+', '_', regex=True)
    df.columns = df.columns.str.strip('_')
    return df

def create_data_loader(X_micro, X_metab, y, batch_size=32, shuffle=True, sampler=None):
    """åˆ›å»ºPyTorchæ•°æ®åŠ è½½å™¨"""
    X_micro_tensor = torch.FloatTensor(X_micro)
    X_metab_tensor = torch.FloatTensor(X_metab)
    y_tensor = torch.FloatTensor(y).unsqueeze(1)
    
    dataset = TensorDataset(X_micro_tensor, X_metab_tensor, y_tensor)
    
    if sampler is not None:
        return DataLoader(dataset, batch_size=batch_size, sampler=sampler)
    else:
        return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)

def plot_training_curves(train_losses, val_aucs):
    """ç»˜åˆ¶è®­ç»ƒæ›²çº¿"""
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))
    
    # æŸå¤±æ›²çº¿
    ax1.plot(train_losses, 'b-', label='Training Loss', linewidth=2)
    ax1.set_xlabel('Epoch')
    ax1.set_ylabel('Loss')
    ax1.set_title('Training Loss Curve')
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    
    # éªŒè¯AUCæ›²çº¿
    ax2.plot(val_aucs, 'r-', label='Validation AUC', linewidth=2)
    ax2.set_xlabel('Epoch')
    ax2.set_ylabel('AUC')
    ax2.set_title('Validation AUC Curve')
    ax2.legend()
    ax2.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig('training_curves_324_allergy.png', dpi=300, bbox_inches='tight')
    plt.show()

def plot_results(y_true, y_pred, y_prob, attention_weights):
    """ç»˜åˆ¶è¯¦ç»†çš„ç»“æœåˆ†æå›¾"""
    fig, axes = plt.subplots(2, 3, figsize=(18, 12))
    
    # 1. æ··æ·†çŸ©é˜µ
    cm = confusion_matrix(y_true, y_pred)
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0,0],
                xticklabels=['Non-allergenic (N)', 'Allergenic (IgE)'],
                yticklabels=['Non-allergenic (N)', 'Allergenic (IgE)'])
    axes[0,0].set_title('Confusion Matrix')
    axes[0,0].set_xlabel('Predicted')
    axes[0,0].set_ylabel('True')
    
    # 2. ROCæ›²çº¿
    fpr, tpr, _ = roc_curve(y_true, y_prob)
    auc = roc_auc_score(y_true, y_prob)
    axes[0,1].plot(fpr, tpr, 'b-', label=f'ROC (AUC = {auc:.3f})', linewidth=2)
    axes[0,1].plot([0, 1], [0, 1], 'r--', alpha=0.7)
    axes[0,1].set_xlabel('False Positive Rate')
    axes[0,1].set_ylabel('True Positive Rate')
    axes[0,1].set_title('ROC Curve - Allergy Classification')
    axes[0,1].legend()
    axes[0,1].grid(True, alpha=0.3)
    
    # 3. é¢„æµ‹æ¦‚ç‡åˆ†å¸ƒ
    axes[0,2].hist(y_prob[y_true == 0], bins=20, alpha=0.7, label='Non-allergenic (N)', color='blue')
    axes[0,2].hist(y_prob[y_true == 1], bins=20, alpha=0.7, label='Allergenic (IgE)', color='red')
    axes[0,2].axvline(x=0.5, color='black', linestyle='--', alpha=0.7, label='Decision Threshold')
    axes[0,2].set_xlabel('Predicted Probability')
    axes[0,2].set_ylabel('Frequency')
    axes[0,2].set_title('Prediction Probability Distribution')
    axes[0,2].legend()
    axes[0,2].grid(True, alpha=0.3)
    
    # 4. æ³¨æ„åŠ›æƒé‡åˆ†æ
    avg_genus_attention = attention_weights[:, 0].mean()
    avg_metab_attention = attention_weights[:, 1].mean()
    modalities = ['Genus\\n(Microbiome)', 'Metabolite']
    avg_weights = [avg_genus_attention, avg_metab_attention]
    colors = ['lightblue', 'lightcoral']
    
    bars = axes[1,0].bar(modalities, avg_weights, color=colors, alpha=0.8)
    axes[1,0].set_ylabel('Average Attention Weight')
    axes[1,0].set_title('Average Attention by Modality')
    axes[1,0].set_ylim(0, 1)
    axes[1,0].grid(axis='y', alpha=0.3)
    
    # æ·»åŠ æ•°å€¼æ ‡ç­¾
    for bar, weight in zip(bars, avg_weights):
        axes[1,0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02, 
                      f'{weight:.3f}', ha='center', va='bottom', fontweight='bold')
    
    # 5. æ³¨æ„åŠ›æƒé‡åˆ†å¸ƒ
    axes[1,1].hist(attention_weights[:, 0], bins=20, alpha=0.7, label='Genus Attention', color='blue')
    axes[1,1].hist(attention_weights[:, 1], bins=20, alpha=0.7, label='Metabolite Attention', color='red')
    axes[1,1].axvline(x=avg_genus_attention, color='blue', linestyle='--', 
                     label=f'Genus Avg: {avg_genus_attention:.3f}')
    axes[1,1].axvline(x=avg_metab_attention, color='red', linestyle='--', 
                     label=f'Metab Avg: {avg_metab_attention:.3f}')
    axes[1,1].set_xlabel('Attention Weight')
    axes[1,1].set_ylabel('Frequency')
    axes[1,1].set_title('Attention Weight Distribution')
    axes[1,1].legend()
    axes[1,1].grid(True, alpha=0.3)
    
    # 6. é¢„æµ‹vsçœŸå®æ ‡ç­¾
    jitter = np.random.normal(0, 0.05, len(y_true))
    scatter = axes[1,2].scatter(y_true + jitter, y_prob, alpha=0.6, c=y_true, cmap='RdYlBu_r', s=30)
    axes[1,2].axhline(y=0.5, color='black', linestyle='--', alpha=0.7, label='Decision Threshold')
    axes[1,2].set_xlabel('True Label')
    axes[1,2].set_ylabel('Predicted Probability')
    axes[1,2].set_title('Predicted Probability vs True Label')
    axes[1,2].set_xticks([0, 1])
    axes[1,2].set_xticklabels(['Non-allergenic (0)', 'Allergenic (1)'])
    axes[1,2].legend()
    axes[1,2].grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig('allergy_classification_results_324.png', dpi=300, bbox_inches='tight')
    plt.show()

def main():
    print("ğŸ§¬ å¤šæ¨¡æ€è¿‡æ•æ€§ç–¾ç—…åˆ†ç±»å™¨ - 324æ•°æ®é›†")
    print("=" * 70)
    print("ğŸ“‹ æ•°æ®è¯´æ˜:")
    print("   â€¢ 1 (IgE): å¼•èµ·è¿‡æ•ååº”çš„æ ·æœ¬")
    print("   â€¢ 0 (N group): éè¿‡æ•æ€§æ ·æœ¬ï¼ˆæ­£å¸¸å¯¹ç…§ç»„ï¼‰")
    print("   â€¢ ä½¿ç”¨æ‰€æœ‰ç‰¹å¾: 567ä¸ªgenus + 324ä¸ªmetabolite = 891ä¸ªç‰¹å¾")
    print("=" * 70)
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"\\nğŸ’» ä½¿ç”¨è®¾å¤‡: {device}")
    
    # åŠ è½½æ•°æ®
    print("\\nğŸ“Š åŠ è½½æ•°æ®...")
    try:
        # X_train = pd.read_excel('324_X_train_source115_genus_metabolite.xlsx', index_col=0)
        # y_train = pd.read_excel('324_y_train_source115_genus_metabolite.xlsx', index_col=0)
        # X_test = pd.read_excel('324_X_test_source203_genus_metabolite.xlsx', index_col=0)
        # y_test = pd.read_excel('324_y_test_source203_genus_metabolite.xlsx', index_col=0)

        X_train = pd.read_excel('324_X_test_source203_genus_metabolite.xlsx', index_col=0)
        y_train = pd.read_excel('324_y_test_source203_genus_metabolite.xlsx', index_col=0)
        X_test = pd.read_excel('324_X_train_source115_genus_metabolite.xlsx', index_col=0)
        y_test = pd.read_excel('324_y_train_source115_genus_metabolite.xlsx', index_col=0)
        
        print(f"âœ… æ•°æ®åŠ è½½æˆåŠŸ")
        print(f"   è®­ç»ƒé›†: {X_train.shape[0]}ä¸ªæ ·æœ¬, {X_train.shape[1]}ä¸ªç‰¹å¾")
        print(f"   æµ‹è¯•é›†: {X_test.shape[0]}ä¸ªæ ·æœ¬, {X_test.shape[1]}ä¸ªç‰¹å¾")
        
    except FileNotFoundError as e:
        print(f"âŒ æ•°æ®æ–‡ä»¶æœªæ‰¾åˆ°: {e}")
        return
    
    # æ¸…ç†åˆ—å
    X_train = clean_column_names(X_train)
    X_test = clean_column_names(X_test)
    
    # æå–æ ‡ç­¾
    y_train_values = y_train.iloc[:, 0].values
    y_test_values = y_test.iloc[:, 0].values
    
    print(f"\\nğŸ“ˆ ç±»åˆ«åˆ†å¸ƒ:")
    train_counts = np.bincount(y_train_values)
    test_counts = np.bincount(y_test_values)
    print(f"   è®­ç»ƒé›†: {train_counts[0]}ä¸ªéè¿‡æ•æ ·æœ¬(N), {train_counts[1]}ä¸ªè¿‡æ•æ ·æœ¬(IgE)")
    print(f"   æµ‹è¯•é›†: {test_counts[0]}ä¸ªéè¿‡æ•æ ·æœ¬(N), {test_counts[1]}ä¸ªè¿‡æ•æ ·æœ¬(IgE)")
    
    # åˆ†ç¦»å¤šæ¨¡æ€ç‰¹å¾
    genus_features = [col for col in X_train.columns if 'genus' in col.lower()]
    metabolite_features = [col for col in X_train.columns if 'metabolite' in col.lower()]
    
    X_train_genus = X_train[genus_features].values
    X_train_metab = X_train[metabolite_features].values
    X_test_genus = X_test[genus_features].values
    X_test_metab = X_test[metabolite_features].values
    
    print(f"\\nğŸ”¬ ç‰¹å¾åˆ†æ:")
    print(f"   Genusç‰¹å¾ (å¾®ç”Ÿç‰©ç»„): {len(genus_features)}ä¸ª")
    print(f"   Metaboliteç‰¹å¾ (ä»£è°¢ç‰©): {len(metabolite_features)}ä¸ª")
    print(f"   æ€»ç‰¹å¾æ•°: {len(genus_features) + len(metabolite_features)}ä¸ª")
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"\\nğŸ’» ä½¿ç”¨è®¾å¤‡: {device}")
    
    # æ•°æ®æ ‡å‡†åŒ–
    print("\\nğŸ“Š æ•°æ®æ ‡å‡†åŒ–...")
    scaler_genus = StandardScaler()
    scaler_metab = StandardScaler()
    
    X_train_genus_scaled = scaler_genus.fit_transform(X_train_genus)
    X_train_metab_scaled = scaler_metab.fit_transform(X_train_metab)
    
    X_test_genus_scaled = scaler_genus.transform(X_test_genus)
    X_test_metab_scaled = scaler_metab.transform(X_test_metab)
    
    print(f"âœ… æ ‡å‡†åŒ–å®Œæˆ:")
    print(f"   Genusç‰¹å¾ç»´åº¦: {X_train_genus_scaled.shape[1]}")
    print(f"   Metaboliteç‰¹å¾ç»´åº¦: {X_train_metab_scaled.shape[1]}")
    
    # åˆ›å»ºæ¨¡å‹
    print("\\nğŸ¤– åˆ›å»ºå¤šæ¨¡æ€æ³¨æ„åŠ›åˆ†ç±»å™¨...")
    model = EnhancedMultimodalClassifier(
        micro_input_dim=X_train_genus_scaled.shape[1],
        metab_input_dim=X_train_metab_scaled.shape[1],
        hidden_dim=128,  # ç”±äºç‰¹å¾å¤šï¼Œä½¿ç”¨æ›´å¤§çš„éšè—å±‚
        dropout_rate=0.4
    ).to(device)
    
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    print(f"æ¨¡å‹å‚æ•°é‡: {total_params:,} (å¯è®­ç»ƒ: {trainable_params:,})")
    
    # å¤„ç†ç±»åˆ«ä¸å¹³è¡¡
    class_weights = compute_class_weight('balanced', classes=np.unique(y_train_values), y=y_train_values)
    class_weight_dict = {0: class_weights[0], 1: class_weights[1]}
    print(f"\\nâš–ï¸ ç±»åˆ«æƒé‡ (å¤„ç†ä¸å¹³è¡¡): éè¿‡æ•={class_weight_dict[0]:.3f}, è¿‡æ•={class_weight_dict[1]:.3f}")
    
    # åŠ æƒé‡‡æ ·å™¨
    sample_weights = [class_weight_dict[label] for label in y_train_values]
    sampler = WeightedRandomSampler(sample_weights, len(sample_weights))
    
    # æ•°æ®åŠ è½½å™¨
    train_loader = create_data_loader(
        X_train_genus_scaled, X_train_metab_scaled, y_train_values,
        batch_size=16, sampler=sampler  # è¾ƒå°çš„batch sizeé€‚åˆé«˜ç»´ç‰¹å¾
    )
    
    # ä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨
    optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-4)
    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=100)
    criterion = nn.BCELoss(weight=torch.FloatTensor([class_weights[1]]).to(device))
    
    # è®­ç»ƒå¾ªç¯
    print("\\nğŸš€ å¼€å§‹è®­ç»ƒ...")
    print("=" * 70)
    train_losses = []
    val_aucs = []
    best_auc = 0
    patience = 15
    no_improve_count = 0
    
    model.train()
    for epoch in range(100):
        epoch_loss = 0
        num_batches = 0
        
        for x_genus, x_metab, labels in train_loader:
            x_genus, x_metab, labels = x_genus.to(device), x_metab.to(device), labels.to(device)
            
            optimizer.zero_grad()
            outputs = model(x_genus, x_metab)
            
            loss = criterion(outputs['classification'], labels)
            
            # L2æ­£åˆ™åŒ–
            l2_reg = sum(p.pow(2.0).sum() for p in model.parameters())
            total_loss = loss + 0.001 * l2_reg
            
            total_loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            optimizer.step()
            
            epoch_loss += loss.item()
            num_batches += 1
        
        avg_loss = epoch_loss / num_batches
        train_losses.append(avg_loss)
        
        # éªŒè¯
        model.eval()
        with torch.no_grad():
            X_test_genus_tensor = torch.FloatTensor(X_test_genus_scaled).to(device)
            X_test_metab_tensor = torch.FloatTensor(X_test_metab_scaled).to(device)
            
            outputs = model(X_test_genus_tensor, X_test_metab_tensor)
            test_pred = outputs['classification'].cpu().numpy().flatten()
            
            val_auc = roc_auc_score(y_test_values, test_pred)
            val_aucs.append(val_auc)
            
            if val_auc > best_auc:
                best_auc = val_auc
                torch.save(model.state_dict(), 'best_allergy_model_324.pth')
                no_improve_count = 0
            else:
                no_improve_count += 1
        
        model.train()
        scheduler.step()
        
        if (epoch + 1) % 10 == 0:
            print(f"  Epoch {epoch+1:3d}: Loss={avg_loss:.4f}, Val_AUC={val_auc:.4f}, Best_AUC={best_auc:.4f}")
        
        # æ—©åœ
        if no_improve_count >= patience:
            print(f"  æ—©åœäºç¬¬{epoch+1}è½® (éªŒè¯AUCè¿ç»­{patience}è½®æœªæå‡)")
            break
    
    # åŠ è½½æœ€ä½³æ¨¡å‹
    model.load_state_dict(torch.load('best_allergy_model_324.pth'))
    
    # æœ€ç»ˆè¯„ä¼° - åˆ†æ‰¹å¤„ç†
    print("\\n" + "="*70)
    print("ğŸ¯ æœ€ç»ˆè¯„ä¼°ç»“æœ - è¿‡æ•æ€§ç–¾ç—…åˆ†ç±»")
    print("="*70)
    
    model.eval()
    with torch.no_grad():
        X_test_genus_tensor = torch.FloatTensor(X_test_genus_scaled).to(device)
        X_test_metab_tensor = torch.FloatTensor(X_test_metab_scaled).to(device)
        
        outputs = model(X_test_genus_tensor, X_test_metab_tensor)
        test_pred_prob = outputs['classification'].cpu().numpy().flatten()
        test_pred_binary = (test_pred_prob > 0.5).astype(int)
        attention_weights = outputs['attention_weights'].cpu().numpy()
    
    # æ€§èƒ½æŒ‡æ ‡
    test_auc = roc_auc_score(y_test_values, test_pred_prob)
    accuracy = (test_pred_binary == y_test_values).mean()
    
    print(f"ğŸ”¥ æµ‹è¯•é›† AUC: {test_auc:.4f}")
    print(f"ğŸ”¥ æµ‹è¯•é›† å‡†ç¡®ç‡: {accuracy:.4f}")
    print(f"ğŸ”¥ æœ€ä½³éªŒè¯ AUC: {best_auc:.4f}")
    
    print("\\nğŸ“Š è¯¦ç»†åˆ†ç±»æŠ¥å‘Š:")
    print(classification_report(y_test_values, test_pred_binary, 
                              target_names=['Non-allergenic (N)', 'Allergenic (IgE)'],
                              digits=4))
    
    # æ³¨æ„åŠ›åˆ†æ
    avg_genus_attention = attention_weights[:, 0].mean()
    avg_metab_attention = attention_weights[:, 1].mean()
    print(f"\\nğŸ¯ æ³¨æ„åŠ›æƒé‡åˆ†æ:")
    print(f"   å¹³å‡Genusæ³¨æ„åŠ›æƒé‡: {avg_genus_attention:.4f}")
    print(f"   å¹³å‡Metaboliteæ³¨æ„åŠ›æƒé‡: {avg_metab_attention:.4f}")
    
    if avg_genus_attention > avg_metab_attention:
        print(f"   â†’ æ¨¡å‹æ›´å…³æ³¨å¾®ç”Ÿç‰©ç»„ç‰¹å¾ (å·®å¼‚: {avg_genus_attention - avg_metab_attention:.4f})")
    else:
        print(f"   â†’ æ¨¡å‹æ›´å…³æ³¨ä»£è°¢ç‰©ç‰¹å¾ (å·®å¼‚: {avg_metab_attention - avg_genus_attention:.4f})")
    
    # å¯è§†åŒ–
    print("\\nğŸ“ˆ ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨...")
    plot_training_curves(train_losses, val_aucs)
    plot_results(y_test_values, test_pred_binary, test_pred_prob, attention_weights)
    
    # ä¿å­˜ç»“æœ
    results_df = pd.DataFrame({
        'sample_id': X_test.index,
        'true_label': y_test_values,
        'true_label_name': ['Non-allergenic' if x == 0 else 'Allergenic' for x in y_test_values],
        'predicted_prob': test_pred_prob,
        'predicted_label': test_pred_binary,
        'predicted_label_name': ['Non-allergenic' if x == 0 else 'Allergenic' for x in test_pred_binary],
        'genus_attention': attention_weights[:, 0],
        'metabolite_attention': attention_weights[:, 1],
        'correct_prediction': (test_pred_binary == y_test_values).astype(int)
    })
    results_df.to_csv('allergy_prediction_results_324.csv', index=False)
    
    # ç»Ÿè®¡é”™è¯¯åˆ†ç±»çš„æ ·æœ¬
    false_positives = results_df[(results_df['true_label'] == 0) & (results_df['predicted_label'] == 1)]
    false_negatives = results_df[(results_df['true_label'] == 1) & (results_df['predicted_label'] == 0)]
    
    print(f"\\nğŸ“‹ é”™è¯¯åˆ†ç±»åˆ†æ:")
    print(f"   å‡é˜³æ€§ (è¯¯åˆ¤ä¸ºè¿‡æ•): {len(false_positives)}ä¸ªæ ·æœ¬")
    print(f"   å‡é˜´æ€§ (æ¼åˆ¤è¿‡æ•): {len(false_negatives)}ä¸ªæ ·æœ¬")
    
    print("\\nâœ… è®­ç»ƒå®Œæˆ!")
    print("ğŸ“ è¾“å‡ºæ–‡ä»¶:")
    print("   â€¢ best_allergy_model_324.pth - æœ€ä½³æ¨¡å‹æƒé‡")
    print("   â€¢ training_curves_324_allergy.png - è®­ç»ƒæ›²çº¿")
    print("   â€¢ allergy_classification_results_324.png - ç»“æœåˆ†æå›¾")
    print("   â€¢ allergy_prediction_results_324.csv - è¯¦ç»†é¢„æµ‹ç»“æœ")

if __name__ == "__main__":
    main()
